{
  "dataSource": <%= data_source.to_json %>,
  "timestampColumn": <%= conf[:raw_input][:timestamp_column].to_json %>,
  "timestampFormat": <%= conf[:raw_input][:timestamp_format].to_json %>,

  "dataSpec": {
    "format": <%= conf[:raw_input][:format].to_json %>,
    "dimensions": <%= conf[:dimensions].to_json %>
  },

  "granularitySpec": {
    "type":"uniform",
    "intervals": <%= intervals.to_json %>,
    "gran": <%= (conf[:segment_output][:segment_granularity] || :hour).to_json %>
  },

  "pathSpec": {
    "type": "static",
    "paths": <%= files.join(',').to_json %>,
    "filePattern": "*"
  },

  "rollupSpec": {
    "aggs": [
<% conf[:metrics].each do |name, data_type|
    if data_type.is_a? Hash %>
      <%= data_type.to_json %>,
<%  else %>
      { "type": <%= data_type.to_json %>, "name": <%= name.to_json %>, "fieldName" : <%= name.to_json %>},
<%  end
   end %>
      { "type":"count", "name": <%= conf[:segment_output][:counter_name].to_json %> }
    ],
    "rollupGranularity": <%= conf[:segment_output][:index_granularity].to_json %>
  },

  "workingPath": <%= "/tmp/druid/#{data_source}/#{Time.now.to_f}".to_json %>,
  "segmentOutputPath": <%= conf[:segment_output][:hdfs_path].to_json %>,
  "leaveIntermediate": false,
  "ignoreInvalidRows" : true,

  "partitionsSpec": {
    "type": "hashed",
    "targetPartitionSize": 5000000
  },

  "updaterJobSpec": {
    "type": "db",
    "connectURI": <%= conf[:database][:uri].to_json %>,
    "user": <%= conf[:database][:user].to_json %>,
    "password": <%= conf[:database][:password].to_json %>,
    "segmentTable": <%= conf[:database][:table].to_json %>
  }
}
